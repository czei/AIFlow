{
  "summary": {
    "total": 14,
    "passed": 9,
    "failed": 5,
    "success_rate": "64.3%",
    "total_duration": "13.30s",
    "timestamp": "2025-07-22T21:38:50.836190+00:00"
  },
  "by_layer": {
    "shell": {
      "total": 5,
      "passed": 5,
      "failed": 0
    },
    "integration": {
      "total": 2,
      "passed": 0,
      "failed": 2
    },
    "unit": {
      "total": 7,
      "passed": 4,
      "failed": 3
    }
  },
  "results": [
    {
      "name": "analyze_logs.sh",
      "success": true,
      "duration": "0.00s",
      "timestamp": "2025-07-22T21:38:37.543456+00:00",
      "metadata": {
        "exit_code": 0,
        "script_path": "analyze_logs.sh",
        "layer": "shell"
      },
      "error": "\ud83d\udcca Phase-Driven Development Log Analysis\n========================================\nProject: .\nLogs: ./.logs\n\n\ud83d\udd52 RECENT ACTIVITY (Last 20 events)\n-----------------------------------\nNo automation.log found\n\n\u274c ERROR SUMMARY\n----------------\nNo errors.log found\n\n\ud83d\udd27 COMMAND EXECUTION SUMMARY\n----------------------------\nNo commands.log found\n\n\u26a1 WORKFLOW PROGRESS\n-------------------\nNo workflow.log found\n\n\ud83d\ude80 PERFORMANCE METRICS\n----------------------\nNo performance.log found\n\n\u2705 QUALITY GATES\n---------------\nNo quality-gates.log found\n\n\ud83d\udd17 CORRELATION ANALYSIS\n----------------------\nNo automation.log found\n\n\ud83d\udccb LOG FILES SUMMARY\n-------------------\n\n\ud83d\udd0d DETAILED ANALYSIS COMMANDS\n-----------------------------\nReal-time monitoring:\n  tail -f ./.logs/automation.log | jq .\n  tail -f ./.logs/errors.log | jq .\n\nSpecific analysis:\n  # Find all failed commands:\n  jq 'select(.details.exit_code != 0)' ./.logs/commands.log\n\n  # Performance analysis:\n  jq 'select(.details.duration_ms > 1000)' ./.logs/performance.log\n\n  # Trace specific correlation ID:\n  grep 'your-correlation-id' ./.logs/*.log | jq .\n\n\u2728 Analysis complete. Use the commands above for deeper investigation.\n"
    },
    {
      "name": "install.sh",
      "success": true,
      "duration": "0.01s",
      "timestamp": "2025-07-22T21:38:37.548967+00:00",
      "metadata": {
        "exit_code": 0,
        "script_path": "install.sh",
        "layer": "shell"
      },
      "error": "\ud83d\ude80 Installing Claude Code Project Management Commands...\n\u2705 Commands installed successfully!\n\nAvailable commands:\n  /user:project:setup <project-name>    - Create new project worktree and structure\n  /user:project:doctor                  - Validate project setup\n  /user:project:start                   - Begin automated development\n  /user:project:status                  - Show project progress\n  /user:project:pause                   - Pause automation\n  /user:project:resume                  - Resume automation\n  /user:project:stop                    - End project cleanly\n  /user:project:advance [phase]         - Force phase advancement\n  /user:project:phase <action>          - Manage phases\n\nNext steps:\n1. Navigate to a git repository\n2. Run: /user:project:setup my-project-name\n3. Customize the phase files created\n4. Run: /user:project:doctor\n5. Run: /user:project:start\n\nNote: These commands are designed for use with --dangerously-skip-permissions\nEnsure you understand the risks and operate in isolated git worktrees.\n\ncp: /Users/czei/claude-project-commands/project/*: No such file or directory\n"
    },
    {
      "name": "tests/integration_test.sh",
      "success": true,
      "duration": "6.32s",
      "timestamp": "2025-07-22T21:38:43.869010+00:00",
      "metadata": {
        "exit_code": 0,
        "script_path": "tests/integration_test.sh",
        "layer": "shell"
      },
      "error": "\ud83e\uddea Phase-Driven Development System Integration Test\n==================================================\n\n\u001b[1;33m\u2139\ufe0f  Setting up test environment...\u001b[0m\nInitialized empty Git repository in /Users/czei/ai-software-project-management/test-output/test-repo/.git/\n[main (root-commit) c4c1824] Initial commit\n 1 file changed, 1 insertion(+)\n create mode 100644 README.md\n\u001b[0;32m\u2705 Test environment ready\u001b[0m\n\nTest 1: Project Setup\n--------------------\n\u001b[1;33m\u2139\ufe0f  Creating git worktree...\u001b[0m\nHEAD is now at c4c1824 Initial commit\n\u001b[0;32m\u2705 Git worktree created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Creating project structure...\u001b[0m\n\u001b[0;32m\u2705 Project structure created\u001b[0m\n\nTest 2: Phase Execution\n----------------------\n\u001b[1;33m\u2139\ufe0f  Executing Phase 01: Setup...\u001b[0m\n\u001b[0;32m\u2705 Phase 01 outputs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Executing Phase 02: Process...\u001b[0m\n\u001b[0;32m\u2705 Phase 02 outputs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Executing Phase 03: Validate...\u001b[0m\n\u001b[0;32m\u2705 Phase 03 validation complete\u001b[0m\n\nTest 3: State Management\n-----------------------\n\u001b[1;33m\u2139\ufe0f  Testing workflow state transitions...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 1: planning\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 2: implementation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 3: validation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 4: review\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 5: refinement\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 6: integration\u001b[0m\n\u001b[0;32m\u2705 Workflow state transitions tested\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Testing quality gate tracking...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: compilation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: testing\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: review\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: integration\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: documentation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: performance\u001b[0m\n\u001b[0;32m\u2705 Quality gates tracked successfully\u001b[0m\n\nTest 4: Logging Infrastructure\n-----------------------------\n\u001b[1;33m\u2139\ufe0f  Creating structured logs...\u001b[0m\n\u001b[0;32m\u2705 Structured logs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Testing log analysis...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Total log entries:        7\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Correlated events: .logs/automation.log:2\n.logs/commands.log:2\n.logs/performance.log:0\n.logs/quality-gates.log:0\u001b[0m\n\u001b[0;32m\u2705 Logging infrastructure verified\u001b[0m\n\nTest 5: Measurable Outcomes\n--------------------------\n\u001b[1;33m\u2139\ufe0f  Verifying all measurable outcomes...\u001b[0m\n\u001b[0;32m\u2705 File exists: output/setup.txt\u001b[0m\n\u001b[0;32m\u2705 File exists: output/config.json\u001b[0m\n\u001b[0;32m\u2705 File exists: output/metrics.json\u001b[0m\n\u001b[0;32m\u2705 File exists: output/processed_data.csv\u001b[0m\n\u001b[0;32m\u2705 File exists: output/validation_report.txt\u001b[0m\n\u001b[0;32m\u2705 File exists: .project-state.json\u001b[0m\n\u001b[0;32m\u2705 File exists: .workflow-state.json\u001b[0m\n\u001b[0;32m\u2705 All expected files created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Checking measurable changes...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  setup.txt has        3 lines\u001b[0m\n\u001b[0;32m\u2705 config.json is valid JSON\u001b[0m\n\u001b[1;33m\u2139\ufe0f  CSV has        4 lines\u001b[0m\n\u001b[0;32m\u2705 Test summary created\u001b[0m\n\n==================================================\n\u001b[0;32m\u2705 All integration tests completed successfully! \ud83c\udf89\u001b[0m\n\n\u001b[1;33m\u2139\ufe0f  Test artifacts available at: /Users/czei/ai-software-project-management/test-output/test-measurable-project\u001b[0m\n\n\n\u001b[1;33m\u2139\ufe0f  Cleaning up...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Cleaning up test directory...\u001b[0m\n\nPreparing worktree (new branch 'feature/test-measurable-project')\n"
    },
    {
      "name": "tests/run_all_tests.sh",
      "success": true,
      "duration": "6.40s",
      "timestamp": "2025-07-22T21:38:50.266369+00:00",
      "metadata": {
        "exit_code": 0,
        "script_path": "tests/run_all_tests.sh",
        "layer": "shell"
      },
      "error": "\ud83e\uddea Phase-Driven Development System - Full Test Suite\n===================================================\n\n\n\u001b[1;33mRunning: Unit Tests\u001b[0m\n----------------------------------------\n\u001b[0;32m\u2705 Unit Tests passed\u001b[0m\n\n\u001b[1;33mRunning: Command Flow Test\u001b[0m\n----------------------------------------\n\ud83e\uddea Claude Code Command Flow Test\n================================\n\n\n--- Executing: /user:project:setup test-project ---\n\ud83d\udd27 Setting up project: test-project\n\u2705 Project structure created at: /Users/czei/ai-software-project-management/test-output/command-test/test-project\n\n--- Executing: /user:project:doctor  ---\n\ud83d\udd0d Running project doctor...\n\u2705 Project structure\n\u2705 State file\n\u2705 Phase files\n\u2705 Output directory\n\u2705 Logs directory\n\n\u2705 All checks passed! Ready to start.\n\n--- Executing: /user:project:start  ---\n\ud83d\ude80 Starting automated development...\n\u2705 Automation started\n\ud83d\udccd Beginning Phase 01...\n\u2705 Phase 01 outputs created\n\n--- Executing: /user:project:status  ---\n\ud83d\udcca Project Status\n================\nProject: test-project\nStatus: active\nCurrent Phase: 01\nWorkflow Step: planning\nAutomation: ACTIVE\nCompleted Phases: None\n\nOutputs Created: 2\n  - phase01.txt\n  - progress.json\n\n--- Executing: /user:project:pause  ---\n\u23f8\ufe0f  Pausing automation...\n\u2705 Automation paused\n\n--- Executing: /user:project:status  ---\n\ud83d\udcca Project Status\n================\nProject: test-project\nStatus: active\nCurrent Phase: 01\nWorkflow Step: planning\nAutomation: PAUSED\nCompleted Phases: None\n\nOutputs Created: 2\n  - phase01.txt\n  - progress.json\n\n--- Executing: /user:project:resume  ---\n\u25b6\ufe0f  Resuming automation...\n\u2705 Automation resumed\n\n--- Executing: /user:project:update  ---\n\ud83d\udcdd Updating project state...\n\u2705 Updated workflow step to: implementation\n\n--- Executing: /user:project:advance  ---\n\u23ed\ufe0f  Advancing phase...\n\u2705 Advanced from Phase 01 to Phase 02\n\u2705 Phase 02 outputs created\n\n--- Executing: /user:project:status  ---\n\ud83d\udcca Project Status\n================\nProject: test-project\nStatus: active\nCurrent Phase: 02\nWorkflow Step: planning\nAutomation: ACTIVE\nCompleted Phases: 01\n\nOutputs Created: 3\n  - phase01.txt\n  - phase02.txt\n  - progress.json\n\n--- Executing: /user:project:stop  ---\n\ud83c\udfc1 Stopping project...\n\n\ud83d\udcca Project Summary:\n  - Phases Completed: 1\n  - Files Created: 3\n  - Status: COMPLETED\n\n\u2705 Command flow test completed!\nTest artifacts at: /Users/czei/ai-software-project-management/test-output/command-test\n\n[Non-interactive mode: Skipping cleanup prompt]\n\u001b[0;32m\u2705 Command Flow Test passed\u001b[0m\n\n\u001b[1;33mRunning: Integration Test\u001b[0m\n----------------------------------------\n\ud83e\uddea Phase-Driven Development System Integration Test\n==================================================\n\n\u001b[1;33m\u2139\ufe0f  Setting up test environment...\u001b[0m\nInitialized empty Git repository in /Users/czei/ai-software-project-management/test-output/test-repo/.git/\n[main (root-commit) 9ac8542] Initial commit\n 1 file changed, 1 insertion(+)\n create mode 100644 README.md\n\u001b[0;32m\u2705 Test environment ready\u001b[0m\n\nTest 1: Project Setup\n--------------------\n\u001b[1;33m\u2139\ufe0f  Creating git worktree...\u001b[0m\nHEAD is now at 9ac8542 Initial commit\n\u001b[0;32m\u2705 Git worktree created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Creating project structure...\u001b[0m\n\u001b[0;32m\u2705 Project structure created\u001b[0m\n\nTest 2: Phase Execution\n----------------------\n\u001b[1;33m\u2139\ufe0f  Executing Phase 01: Setup...\u001b[0m\n\u001b[0;32m\u2705 Phase 01 outputs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Executing Phase 02: Process...\u001b[0m\n\u001b[0;32m\u2705 Phase 02 outputs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Executing Phase 03: Validate...\u001b[0m\n\u001b[0;32m\u2705 Phase 03 validation complete\u001b[0m\n\nTest 3: State Management\n-----------------------\n\u001b[1;33m\u2139\ufe0f  Testing workflow state transitions...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 1: planning\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 2: implementation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 3: validation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 4: review\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 5: refinement\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 6: integration\u001b[0m\n\u001b[0;32m\u2705 Workflow state transitions tested\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Testing quality gate tracking...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: compilation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: testing\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: review\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: integration\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: documentation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: performance\u001b[0m\n\u001b[0;32m\u2705 Quality gates tracked successfully\u001b[0m\n\nTest 4: Logging Infrastructure\n-----------------------------\n\u001b[1;33m\u2139\ufe0f  Creating structured logs...\u001b[0m\n\u001b[0;32m\u2705 Structured logs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Testing log analysis...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Total log entries:        7\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Correlated events: .logs/automation.log:2\n.logs/commands.log:2\n.logs/performance.log:0\n.logs/quality-gates.log:0\u001b[0m\n\u001b[0;32m\u2705 Logging infrastructure verified\u001b[0m\n\nTest 5: Measurable Outcomes\n--------------------------\n\u001b[1;33m\u2139\ufe0f  Verifying all measurable outcomes...\u001b[0m\n\u001b[0;32m\u2705 File exists: output/setup.txt\u001b[0m\n\u001b[0;32m\u2705 File exists: output/config.json\u001b[0m\n\u001b[0;32m\u2705 File exists: output/metrics.json\u001b[0m\n\u001b[0;32m\u2705 File exists: output/processed_data.csv\u001b[0m\n\u001b[0;32m\u2705 File exists: output/validation_report.txt\u001b[0m\n\u001b[0;32m\u2705 File exists: .project-state.json\u001b[0m\n\u001b[0;32m\u2705 File exists: .workflow-state.json\u001b[0m\n\u001b[0;32m\u2705 All expected files created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Checking measurable changes...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  setup.txt has        3 lines\u001b[0m\n\u001b[0;32m\u2705 config.json is valid JSON\u001b[0m\n\u001b[1;33m\u2139\ufe0f  CSV has        4 lines\u001b[0m\n\u001b[0;32m\u2705 Test summary created\u001b[0m\n\n==================================================\n\u001b[0;32m\u2705 All integration tests completed successfully! \ud83c\udf89\u001b[0m\n\n\u001b[1;33m\u2139\ufe0f  Test artifacts available at: /Users/czei/ai-software-project-management/test-output/test-measurable-project\u001b[0m\n\n\n\u001b[1;33m\u2139\ufe0f  Cleaning up...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Cleaning up test directory...\u001b[0m\n\u001b[0;32m\u2705 Integration Test passed\u001b[0m\n\n===================================================\nTest Summary:\n  \u001b[0;32mPassed: 3\u001b[0m\n  \u001b[0;31mFailed: 0\u001b[0m\n\n\u001b[0;32m\u2705 All tests passed!\u001b[0m\n\ntest_log_file_creation (__main__.TestLoggingInfrastructure.test_log_file_creation)\nTest that all log files are created ... ok\ntest_performance_metrics (__main__.TestLoggingInfrastructure.test_performance_metrics)\nTest performance logging ... /Users/czei/ai-software-project-management/tests/test_phases.py:524: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_structured_logging (__main__.TestLoggingInfrastructure.test_structured_logging)\nTest JSON structured log format ... /Users/czei/ai-software-project-management/tests/test_phases.py:493: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_automation_control (__main__.TestPhaseDrivenDevelopment.test_automation_control)\nTest automation pause/resume functionality ... /Users/czei/ai-software-project-management/tests/test_phases.py:181: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"started\": datetime.utcnow().isoformat() + \"Z\"\nok\ntest_error_recovery (__main__.TestPhaseDrivenDevelopment.test_error_recovery)\nTest error handling and recovery ... ok\ntest_phase_advancement (__main__.TestPhaseDrivenDevelopment.test_phase_advancement)\nTest phase progression logic ... ok\ntest_phase_file_structure (__main__.TestPhaseDrivenDevelopment.test_phase_file_structure)\nTest that phase files have required sections ... ok\ntest_phase_operations (__main__.TestPhaseDrivenDevelopment.test_phase_operations)\nTest actual phase operations (file creation/modification) ... /Users/czei/ai-software-project-management/tests/test_phases.py:311: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  initial_data = {\"version\": \"1.0\", \"created\": datetime.utcnow().isoformat()}\n/Users/czei/ai-software-project-management/tests/test_phases.py:313: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  log_file.write_text(f\"[{datetime.utcnow().isoformat()}] Project initialized\\n\")\n/Users/czei/ai-software-project-management/tests/test_phases.py:328: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  f.write(f\"[{datetime.utcnow().isoformat()}] Processing started\\n\")\n/Users/czei/ai-software-project-management/tests/test_phases.py:342: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  Generated: {datetime.utcnow().isoformat()}\n/Users/czei/ai-software-project-management/tests/test_phases.py:369: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"completed\": datetime.utcnow().isoformat()\nok\ntest_quality_gates (__main__.TestPhaseDrivenDevelopment.test_quality_gates)\nTest quality gate tracking ... ok\ntest_state_management (__main__.TestPhaseDrivenDevelopment.test_state_management)\nTest project state file operations ... ok\ntest_workflow_progression (__main__.TestPhaseDrivenDevelopment.test_workflow_progression)\nTest 6-step workflow state transitions ... ok\n\n----------------------------------------------------------------------\nRan 11 tests in 0.013s\n\nOK\n/Users/czei/ai-software-project-management/tests/test_command_flow.py:77: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"started\": datetime.utcnow().isoformat() + \"Z\"\n/Users/czei/ai-software-project-management/tests/test_command_flow.py:304: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  phase_file.write_text(f\"Phase {phase_num} executed at {datetime.utcnow().isoformat()}\\n\")\n/Users/czei/ai-software-project-management/tests/test_command_flow.py:314: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"completed\": datetime.utcnow().isoformat(),\n/Users/czei/ai-software-project-management/tests/test_command_flow.py:283: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  state[\"completed\"] = datetime.utcnow().isoformat() + \"Z\"\nPreparing worktree (new branch 'feature/test-measurable-project')\n"
    },
    {
      "name": "tests/run_unit_tests.sh",
      "success": true,
      "duration": "0.10s",
      "timestamp": "2025-07-22T21:38:50.371086+00:00",
      "metadata": {
        "exit_code": 0,
        "script_path": "tests/run_unit_tests.sh",
        "layer": "shell"
      },
      "error": "Running unit tests for deterministic components...\n================================================\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u274c FAILED - test.sh (0.00s)\n\nErrors:\nTest failed\n\n============================================================\nRunning: test.sh\n============================================================\n\n\ud83d\udea8 PERMISSION ERROR - test.sh\nFix with: chmod +x /test/project/test.sh\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u2705 PASSED - test.sh (0.00s)\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u23f1\ufe0f  TIMEOUT - test.sh (300s)\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u2705 PASSED - test.sh (0.00s)\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 3\nPassed:      2 \u2705\nFailed:      1 \u274c\nSuccess Rate: 66.7%\nTotal Time:   4.30s\n\nBy Layer:\n  shell: 1/2 passed\n  python: 1/1 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250722_173850.json\n============================================================\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nLoading plugins from: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\n\nSkipping disabled layer: test\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nRunning shell tests\n============================================================\nFound 2 shell test(s)\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 2\nPassed:      1 \u2705\nFailed:      1 \u274c\nSuccess Rate: 50.0%\nTotal Time:   3.00s\n\nBy Layer:\n  shell: 1/2 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250101_000000.json\n============================================================\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 2\nPassed:      1 \u2705\nFailed:      1 \u274c\nSuccess Rate: 50.0%\nTotal Time:   3.00s\n\nBy Layer:\n  shell: 1/2 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250101_000000.json\n============================================================\n\n\u2705 All unit tests passed!\n\ntest_log_event_console_output (test_basic_logger.TestBasicLogger.test_log_event_console_output)\nTest log event prints to console ... /Users/czei/ai-software-project-management/logged_secure_shell.py:59: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_log_event_invalid_json_state (test_basic_logger.TestBasicLogger.test_log_event_invalid_json_state)\nTest log event handles invalid JSON in state file ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\nok\ntest_log_event_missing_state_file (test_basic_logger.TestBasicLogger.test_log_event_missing_state_file)\nTest log event handles missing state file gracefully ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\nok\ntest_log_event_structure (test_basic_logger.TestBasicLogger.test_log_event_structure)\nTest log event creates proper JSON structure ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event - {\"key\":\"value\"}\nok\ntest_log_event_unknown_category (test_basic_logger.TestBasicLogger.test_log_event_unknown_category)\nTest unknown category defaults to automation log ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] unknown_category: test_event\nok\ntest_log_file_selection (test_basic_logger.TestBasicLogger.test_log_file_selection)\nTest correct log file is selected based on category ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\n[INFO] workflow: test_event\n[INFO] commands: test_event\n[INFO] errors: test_event\n[INFO] performance: test_event\nok\ntest_logger_auto_correlation_id (test_basic_logger.TestBasicLogger.test_logger_auto_correlation_id)\nTest logger generates correlation ID when not provided ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\nok\ntest_logger_initialization (test_basic_logger.TestBasicLogger.test_logger_initialization)\nTest logger initializes with correct attributes ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"custom-id\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\nok\ntest_load_project_state_file_not_found (test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_file_not_found)\nTest project state loading when file not found ... ok\ntest_load_project_state_invalid_json (test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_invalid_json)\nTest project state loading with invalid JSON ... ok\ntest_load_project_state_success (test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_success)\nTest successful project state loading ... ok\ntest_shell_initialization (test_logged_secure_shell.TestLoggedSecureShell.test_shell_initialization)\nTest shell initializes with correct attributes ... ok\ntest_validate_command_phase_args_preview (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_args_preview)\nTest args preview in validation logging ... ok\ntest_validate_command_phase_implementation (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_implementation)\nTest command validation for implementation phase ... ok\ntest_validate_command_phase_performance_tracking (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_performance_tracking)\nTest that command validation tracks performance ... ok\ntest_validate_command_phase_planning (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_planning)\nTest command validation for planning phase ... ok\ntest_validate_command_phase_review (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_review)\nTest command validation for review phase ... ok\ntest_validate_command_phase_unknown (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_unknown)\nTest command validation for unknown phase ... ok\ntest_validate_command_phase_validation (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_validation)\nTest command validation for validation phase ... ok\ntest_validate_command_phase_validation_result (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_validation_result)\nTest validation result logging ... ok\ntest_main_empty_command (test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_empty_command)\nTest main with empty command string ... \u274c Empty command provided\nok\ntest_main_invalid_command_string (test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_invalid_command_string)\nTest main with invalid command string ... \u274c Invalid command string: No closing quotation\nok\ntest_main_no_command (test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_no_command)\nTest main with no command provided ... Usage: logged_secure_shell <command>\nok\ntest_discover_tests (test_test_runner_v2.TestShellTestLayer.test_discover_tests)\nTest test discovery ... ok\ntest_discover_tests_multiple_patterns (test_test_runner_v2.TestShellTestLayer.test_discover_tests_multiple_patterns)\nTest discovery with multiple patterns ... ok\ntest_run_test_failure (test_test_runner_v2.TestShellTestLayer.test_run_test_failure)\nTest failed test execution ... ok\ntest_run_test_permission_error (test_test_runner_v2.TestShellTestLayer.test_run_test_permission_error)\nTest permission error handling ... ok\ntest_run_test_success (test_test_runner_v2.TestShellTestLayer.test_run_test_success)\nTest successful test execution ... ok\ntest_run_test_timeout (test_test_runner_v2.TestShellTestLayer.test_run_test_timeout)\nTest test execution timeout ... ok\ntest_run_test_with_input (test_test_runner_v2.TestShellTestLayer.test_run_test_with_input)\nTest that input=\"\" is passed to prevent hangs ... ok\ntest_context_defaults (test_test_runner_v2.TestTestContext.test_context_defaults)\nTest TestContext default values ... ok\ntest_context_initialization (test_test_runner_v2.TestTestContext.test_context_initialization)\nTest TestContext initializes correctly ... ok\ntest_get_layer (test_test_runner_v2.TestTestLayerRegistry.test_get_layer)\nTest getting registered layer ... ok\ntest_get_nonexistent_layer (test_test_runner_v2.TestTestLayerRegistry.test_get_nonexistent_layer)\nTest getting non-existent layer returns None ... ok\ntest_list_layers (test_test_runner_v2.TestTestLayerRegistry.test_list_layers)\nTest listing all layers ... ok\ntest_register_layer (test_test_runner_v2.TestTestLayerRegistry.test_register_layer)\nTest layer registration ... ok\ntest_result_initialization (test_test_runner_v2.TestTestResult.test_result_initialization)\nTest TestResult initializes correctly ... ok\ntest_result_to_dict (test_test_runner_v2.TestTestResult.test_result_to_dict)\nTest TestResult.to_dict method ... ok\ntest_calculate_summary (test_test_runner_v2.TestTestRunner.test_calculate_summary)\nTest summary calculation from results ... ok\ntest_initialization (test_test_runner_v2.TestTestRunner.test_initialization)\nTest runner initialization ... ok\ntest_load_config_missing (test_test_runner_v2.TestTestRunner.test_load_config_missing)\nTest config loading with missing file ... ok\ntest_load_config_success (test_test_runner_v2.TestTestRunner.test_load_config_success)\nTest successful config loading ... ok\ntest_run_layer_disabled (test_test_runner_v2.TestTestRunner.test_run_layer_disabled)\nTest running disabled layer ... ok\ntest_run_layer_success (test_test_runner_v2.TestTestRunner.test_run_layer_success)\nTest running enabled layer ... ok\ntest_save_results (test_test_runner_v2.TestTestRunner.test_save_results)\nTest saving results to JSON ... ok\n\n----------------------------------------------------------------------\nRan 45 tests in 0.020s\n\nOK\n"
    },
    {
      "name": "tests/integration/test_ai_workflow_integration.py",
      "success": false,
      "duration": "0.04s",
      "timestamp": "2025-07-22T21:38:50.411286+00:00",
      "metadata": {
        "exit_code": 1,
        "test_file": "tests/integration/test_ai_workflow_integration.py",
        "module_name": "tests.integration.test_ai_workflow_integration",
        "layer": "integration",
        "test_count": 7,
        "failures": 1,
        "errors": 0,
        "uses_mock_provider": true
      },
      "error": "\ntest_ai_guided_refactoring (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_ai_guided_refactoring)\nTest AI-guided code refactoring workflow ... ok\ntest_complete_development_workflow (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_complete_development_workflow)\nTest complete workflow from setup to implementation ... FAIL\ntest_complex_multi_step_workflow (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_complex_multi_step_workflow)\nTest complex workflow with multiple interdependent steps ... ok\ntest_error_recovery_workflow (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_error_recovery_workflow)\nTest AI-assisted error recovery workflow ... ok\ntest_iterative_development_with_feedback (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_iterative_development_with_feedback)\nTest iterative development with AI feedback loop ... ok\ntest_multi_phase_state_persistence (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_multi_phase_state_persistence)\nTest that state persists across multiple phases ... ok\ntest_workflow_with_failures (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_workflow_with_failures)\nTest workflow behavior with simulated failures ... ok\n\n======================================================================\nFAIL: test_complete_development_workflow (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_complete_development_workflow)\nTest complete workflow from setup to implementation\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/czei/ai-software-project-management/tests/integration/test_ai_workflow_integration.py\", line 73, in test_complete_development_workflow\n    self.assertEqual(review_response[\"type\"], \"code_review\")\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: 'code_implementation' != 'code_review'\n- code_implementation\n+ code_review\n\n\n----------------------------------------------------------------------\nRan 7 tests in 0.003s\n\nFAILED (failures=1)\n"
    },
    {
      "name": "tests/integration/test_mock_claude_integration.py",
      "success": false,
      "duration": "0.06s",
      "timestamp": "2025-07-22T21:38:50.471503+00:00",
      "metadata": {
        "exit_code": 1,
        "test_file": "tests/integration/test_mock_claude_integration.py",
        "module_name": "tests.integration.test_mock_claude_integration",
        "layer": "integration",
        "test_count": 25,
        "failures": 0,
        "errors": 1,
        "uses_mock_provider": true,
        "mock_provider_active": true
      },
      "error": "\ntest_call_history_tracking (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_call_history_tracking)\nTest that call history is properly tracked ... ok\ntest_clear_history (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_clear_history)\nTest clearing call history ... ok\ntest_deterministic_code_implementation (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_deterministic_code_implementation)\nTest deterministic responses for code implementation ... ok\ntest_deterministic_code_review (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_deterministic_code_review)\nTest deterministic responses for code review ... ok\ntest_deterministic_error_analysis (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_deterministic_error_analysis)\nTest deterministic responses for error analysis ... ok\ntest_deterministic_project_setup (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_deterministic_project_setup)\nTest deterministic responses for project setup ... ok\ntest_failure_mode (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_failure_mode)\nTest failure response mode ... ok\ntest_inject_custom_response (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_inject_custom_response)\nTest injecting custom responses ... ok\ntest_random_mode (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_random_mode)\nTest random response mode ... ok\ntest_completed_tasks_tracking (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_completed_tasks_tracking)\nTest tracking of completed tasks ... ok\ntest_context_accumulation (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_context_accumulation)\nTest that context accumulates across calls ... ok\ntest_error_count_tracking (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_error_count_tracking)\nTest error counting ... ok\ntest_initial_state (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_initial_state)\nTest initial state values ... ok\ntest_phase_transition_tracking (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_phase_transition_tracking)\nTest that phase transitions are tracked ... ok\ntest_reset_state (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_reset_state)\nTest state reset functionality ... ok\ntest_state_included_in_response (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_state_included_in_response)\nTest that state is included in responses ... ok\ntest_ai_error_recovery_flow (tests.integration.test_mock_claude_integration.TestMockClaudeWithLoggedShell.test_ai_error_recovery_flow)\nTest error recovery flow with AI assistance ... /Users/czei/ai-software-project-management/logged_secure_shell.py:59: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n[INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"5598301f-0d51-43f7-a4dd-cbf67f1cb070\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: secure_shell_initialized - {\"workdir\":\"/test/project\",\"pid\":22649,\"command_args\":[\"python -m unittest\",\"tests.integration.test_mock_claude_integration\",\"-v\"]}\nok\ntest_ai_guided_command_validation (tests.integration.test_mock_claude_integration.TestMockClaudeWithLoggedShell.test_ai_guided_command_validation)\nTest command validation with AI guidance ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"2b1bf37d-27f0-4fc9-943f-2012b7ee4671\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: secure_shell_initialized - {\"workdir\":\"/test/project\",\"pid\":22649,\"command_args\":[\"python -m unittest\",\"tests.integration.test_mock_claude_integration\",\"-v\"]}\nERROR\ntest_ai_guided_workflow (tests.integration.test_mock_claude_integration.TestMockClaudeWithLoggedShell.test_ai_guided_workflow)\nTest complete workflow with AI guidance ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"31a1c71b-61ea-40a2-9d55-d54d14227c9a\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: secure_shell_initialized - {\"workdir\":\"/test/project\",\"pid\":22649,\"command_args\":[\"python -m unittest\",\"tests.integration.test_mock_claude_integration\",\"-v\"]}\nok\ntest_concurrent_queries (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_concurrent_queries)\nTest handling of multiple queries ... ok\ntest_empty_prompt (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_empty_prompt)\nTest handling of empty prompt ... ok\ntest_invalid_response_mode (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_invalid_response_mode)\nTest setting invalid response mode ... ok\ntest_special_characters_in_prompt (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_special_characters_in_prompt)\nTest handling of special characters ... ok\ntest_unicode_in_prompt (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_unicode_in_prompt)\nTest handling of unicode characters ... ok\ntest_very_long_prompt (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_very_long_prompt)\nTest handling of very long prompt ... ok\n\n======================================================================\nERROR: test_ai_guided_command_validation (tests.integration.test_mock_claude_integration.TestMockClaudeWithLoggedShell.test_ai_guided_command_validation)\nTest command validation with AI guidance\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/czei/ai-software-project-management/tests/integration/test_mock_claude_integration.py\", line 209, in test_ai_guided_command_validation\n    is_valid = self.shell.validate_command_for_phase(\"ls -la\", \"implementation\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'LoggedSecureShell' object has no attribute 'validate_command_for_phase'. Did you mean: 'validate_command_phase'?\n\n----------------------------------------------------------------------\nRan 25 tests in 0.005s\n\nFAILED (errors=1)\n"
    },
    {
      "name": "tests/integration/test_ai_workflow_integration.py",
      "success": false,
      "duration": "0.04s",
      "timestamp": "2025-07-22T21:38:50.514809+00:00",
      "metadata": {
        "exit_code": 1,
        "test_file": "tests/integration/test_ai_workflow_integration.py",
        "module_name": "tests.integration.test_ai_workflow_integration",
        "layer": "unit",
        "test_count": 7,
        "failures": 1,
        "errors": 0
      },
      "error": "\ntest_ai_guided_refactoring (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_ai_guided_refactoring)\nTest AI-guided code refactoring workflow ... ok\ntest_complete_development_workflow (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_complete_development_workflow)\nTest complete workflow from setup to implementation ... FAIL\ntest_complex_multi_step_workflow (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_complex_multi_step_workflow)\nTest complex workflow with multiple interdependent steps ... ok\ntest_error_recovery_workflow (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_error_recovery_workflow)\nTest AI-assisted error recovery workflow ... ok\ntest_iterative_development_with_feedback (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_iterative_development_with_feedback)\nTest iterative development with AI feedback loop ... ok\ntest_multi_phase_state_persistence (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_multi_phase_state_persistence)\nTest that state persists across multiple phases ... ok\ntest_workflow_with_failures (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_workflow_with_failures)\nTest workflow behavior with simulated failures ... ok\n\n======================================================================\nFAIL: test_complete_development_workflow (tests.integration.test_ai_workflow_integration.TestAIWorkflowIntegration.test_complete_development_workflow)\nTest complete workflow from setup to implementation\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/czei/ai-software-project-management/tests/integration/test_ai_workflow_integration.py\", line 73, in test_complete_development_workflow\n    self.assertEqual(review_response[\"type\"], \"code_review\")\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: 'code_implementation' != 'code_review'\n- code_implementation\n+ code_review\n\n\n----------------------------------------------------------------------\nRan 7 tests in 0.003s\n\nFAILED (failures=1)\n"
    },
    {
      "name": "tests/integration/test_mock_claude_integration.py",
      "success": false,
      "duration": "0.06s",
      "timestamp": "2025-07-22T21:38:50.573462+00:00",
      "metadata": {
        "exit_code": 1,
        "test_file": "tests/integration/test_mock_claude_integration.py",
        "module_name": "tests.integration.test_mock_claude_integration",
        "layer": "unit",
        "test_count": 25,
        "failures": 0,
        "errors": 1
      },
      "error": "\ntest_call_history_tracking (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_call_history_tracking)\nTest that call history is properly tracked ... ok\ntest_clear_history (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_clear_history)\nTest clearing call history ... ok\ntest_deterministic_code_implementation (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_deterministic_code_implementation)\nTest deterministic responses for code implementation ... ok\ntest_deterministic_code_review (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_deterministic_code_review)\nTest deterministic responses for code review ... ok\ntest_deterministic_error_analysis (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_deterministic_error_analysis)\nTest deterministic responses for error analysis ... ok\ntest_deterministic_project_setup (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_deterministic_project_setup)\nTest deterministic responses for project setup ... ok\ntest_failure_mode (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_failure_mode)\nTest failure response mode ... ok\ntest_inject_custom_response (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_inject_custom_response)\nTest injecting custom responses ... ok\ntest_random_mode (tests.integration.test_mock_claude_integration.TestMockClaudeProviderBasic.test_random_mode)\nTest random response mode ... ok\ntest_completed_tasks_tracking (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_completed_tasks_tracking)\nTest tracking of completed tasks ... ok\ntest_context_accumulation (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_context_accumulation)\nTest that context accumulates across calls ... ok\ntest_error_count_tracking (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_error_count_tracking)\nTest error counting ... ok\ntest_initial_state (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_initial_state)\nTest initial state values ... ok\ntest_phase_transition_tracking (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_phase_transition_tracking)\nTest that phase transitions are tracked ... ok\ntest_reset_state (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_reset_state)\nTest state reset functionality ... ok\ntest_state_included_in_response (tests.integration.test_mock_claude_integration.TestMockClaudeProviderWithState.test_state_included_in_response)\nTest that state is included in responses ... ok\ntest_ai_error_recovery_flow (tests.integration.test_mock_claude_integration.TestMockClaudeWithLoggedShell.test_ai_error_recovery_flow)\nTest error recovery flow with AI assistance ... /Users/czei/ai-software-project-management/logged_secure_shell.py:59: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n[INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"91c5d820-556f-4787-bbc7-df2c1bf7c5de\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: secure_shell_initialized - {\"workdir\":\"/test/project\",\"pid\":22651,\"command_args\":[\"python -m unittest\",\"tests.integration.test_mock_claude_integration\",\"-v\"]}\nok\ntest_ai_guided_command_validation (tests.integration.test_mock_claude_integration.TestMockClaudeWithLoggedShell.test_ai_guided_command_validation)\nTest command validation with AI guidance ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"ae7c2f5a-b7e3-41ec-b63c-93079e95f70f\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: secure_shell_initialized - {\"workdir\":\"/test/project\",\"pid\":22651,\"command_args\":[\"python -m unittest\",\"tests.integration.test_mock_claude_integration\",\"-v\"]}\nERROR\ntest_ai_guided_workflow (tests.integration.test_mock_claude_integration.TestMockClaudeWithLoggedShell.test_ai_guided_workflow)\nTest complete workflow with AI guidance ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"50c2f95e-03d8-458b-94d8-4f49f0c4f997\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: secure_shell_initialized - {\"workdir\":\"/test/project\",\"pid\":22651,\"command_args\":[\"python -m unittest\",\"tests.integration.test_mock_claude_integration\",\"-v\"]}\nok\ntest_concurrent_queries (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_concurrent_queries)\nTest handling of multiple queries ... ok\ntest_empty_prompt (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_empty_prompt)\nTest handling of empty prompt ... ok\ntest_invalid_response_mode (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_invalid_response_mode)\nTest setting invalid response mode ... ok\ntest_special_characters_in_prompt (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_special_characters_in_prompt)\nTest handling of special characters ... ok\ntest_unicode_in_prompt (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_unicode_in_prompt)\nTest handling of unicode characters ... ok\ntest_very_long_prompt (tests.integration.test_mock_claude_integration.TestMockProviderEdgeCases.test_very_long_prompt)\nTest handling of very long prompt ... ok\n\n======================================================================\nERROR: test_ai_guided_command_validation (tests.integration.test_mock_claude_integration.TestMockClaudeWithLoggedShell.test_ai_guided_command_validation)\nTest command validation with AI guidance\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/czei/ai-software-project-management/tests/integration/test_mock_claude_integration.py\", line 209, in test_ai_guided_command_validation\n    is_valid = self.shell.validate_command_for_phase(\"ls -la\", \"implementation\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'LoggedSecureShell' object has no attribute 'validate_command_for_phase'. Did you mean: 'validate_command_phase'?\n\n----------------------------------------------------------------------\nRan 25 tests in 0.005s\n\nFAILED (errors=1)\n"
    },
    {
      "name": "tests/test_command_flow.py",
      "success": false,
      "duration": "0.03s",
      "timestamp": "2025-07-22T21:38:50.605545+00:00",
      "metadata": {
        "exit_code": 5,
        "test_file": "tests/test_command_flow.py",
        "module_name": "tests.test_command_flow",
        "layer": "unit",
        "test_count": 0,
        "failures": 0,
        "errors": 0
      },
      "error": "\n\n----------------------------------------------------------------------\nRan 0 tests in 0.000s\n\nNO TESTS RAN\n"
    },
    {
      "name": "tests/test_phases.py",
      "success": true,
      "duration": "0.05s",
      "timestamp": "2025-07-22T21:38:50.654324+00:00",
      "metadata": {
        "exit_code": 0,
        "test_file": "tests/test_phases.py",
        "module_name": "tests.test_phases",
        "layer": "unit",
        "test_count": 11,
        "failures": 0,
        "errors": 0
      },
      "error": "\ntest_log_file_creation (tests.test_phases.TestLoggingInfrastructure.test_log_file_creation)\nTest that all log files are created ... ok\ntest_performance_metrics (tests.test_phases.TestLoggingInfrastructure.test_performance_metrics)\nTest performance logging ... /Users/czei/ai-software-project-management/tests/test_phases.py:524: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_structured_logging (tests.test_phases.TestLoggingInfrastructure.test_structured_logging)\nTest JSON structured log format ... /Users/czei/ai-software-project-management/tests/test_phases.py:493: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_automation_control (tests.test_phases.TestPhaseDrivenDevelopment.test_automation_control)\nTest automation pause/resume functionality ... /Users/czei/ai-software-project-management/tests/test_phases.py:181: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"started\": datetime.utcnow().isoformat() + \"Z\"\nok\ntest_error_recovery (tests.test_phases.TestPhaseDrivenDevelopment.test_error_recovery)\nTest error handling and recovery ... ok\ntest_phase_advancement (tests.test_phases.TestPhaseDrivenDevelopment.test_phase_advancement)\nTest phase progression logic ... ok\ntest_phase_file_structure (tests.test_phases.TestPhaseDrivenDevelopment.test_phase_file_structure)\nTest that phase files have required sections ... ok\ntest_phase_operations (tests.test_phases.TestPhaseDrivenDevelopment.test_phase_operations)\nTest actual phase operations (file creation/modification) ... /Users/czei/ai-software-project-management/tests/test_phases.py:311: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  initial_data = {\"version\": \"1.0\", \"created\": datetime.utcnow().isoformat()}\n/Users/czei/ai-software-project-management/tests/test_phases.py:313: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  log_file.write_text(f\"[{datetime.utcnow().isoformat()}] Project initialized\\n\")\n/Users/czei/ai-software-project-management/tests/test_phases.py:328: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  f.write(f\"[{datetime.utcnow().isoformat()}] Processing started\\n\")\n/Users/czei/ai-software-project-management/tests/test_phases.py:342: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  Generated: {datetime.utcnow().isoformat()}\n/Users/czei/ai-software-project-management/tests/test_phases.py:369: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"completed\": datetime.utcnow().isoformat()\nok\ntest_quality_gates (tests.test_phases.TestPhaseDrivenDevelopment.test_quality_gates)\nTest quality gate tracking ... ok\ntest_state_management (tests.test_phases.TestPhaseDrivenDevelopment.test_state_management)\nTest project state file operations ... ok\ntest_workflow_progression (tests.test_phases.TestPhaseDrivenDevelopment.test_workflow_progression)\nTest 6-step workflow state transitions ... ok\n\n----------------------------------------------------------------------\nRan 11 tests in 0.011s\n\nOK\n"
    },
    {
      "name": "tests/unit/test_basic_logger.py",
      "success": true,
      "duration": "0.07s",
      "timestamp": "2025-07-22T21:38:50.719506+00:00",
      "metadata": {
        "exit_code": 0,
        "test_file": "tests/unit/test_basic_logger.py",
        "module_name": "tests.unit.test_basic_logger",
        "layer": "unit",
        "test_count": 8,
        "failures": 0,
        "errors": 0
      },
      "error": "\ntest_log_event_console_output (tests.unit.test_basic_logger.TestBasicLogger.test_log_event_console_output)\nTest log event prints to console ... /Users/czei/ai-software-project-management/logged_secure_shell.py:59: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_log_event_invalid_json_state (tests.unit.test_basic_logger.TestBasicLogger.test_log_event_invalid_json_state)\nTest log event handles invalid JSON in state file ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\nok\ntest_log_event_missing_state_file (tests.unit.test_basic_logger.TestBasicLogger.test_log_event_missing_state_file)\nTest log event handles missing state file gracefully ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\nok\ntest_log_event_structure (tests.unit.test_basic_logger.TestBasicLogger.test_log_event_structure)\nTest log event creates proper JSON structure ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event - {\"key\":\"value\"}\nok\ntest_log_event_unknown_category (tests.unit.test_basic_logger.TestBasicLogger.test_log_event_unknown_category)\nTest unknown category defaults to automation log ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] unknown_category: test_event\nok\ntest_log_file_selection (tests.unit.test_basic_logger.TestBasicLogger.test_log_file_selection)\nTest correct log file is selected based on category ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\n[INFO] workflow: test_event\n[INFO] commands: test_event\n[INFO] errors: test_event\n[INFO] performance: test_event\nok\ntest_logger_auto_correlation_id (tests.unit.test_basic_logger.TestBasicLogger.test_logger_auto_correlation_id)\nTest logger generates correlation ID when not provided ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\nok\ntest_logger_initialization (tests.unit.test_basic_logger.TestBasicLogger.test_logger_initialization)\nTest logger initializes with correct attributes ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"custom-id\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\nok\n\n----------------------------------------------------------------------\nRan 8 tests in 0.012s\n\nOK\n"
    },
    {
      "name": "tests/unit/test_logged_secure_shell.py",
      "success": true,
      "duration": "0.06s",
      "timestamp": "2025-07-22T21:38:50.774729+00:00",
      "metadata": {
        "exit_code": 0,
        "test_file": "tests/unit/test_logged_secure_shell.py",
        "module_name": "tests.unit.test_logged_secure_shell",
        "layer": "unit",
        "test_count": 15,
        "failures": 0,
        "errors": 0
      },
      "error": "\ntest_load_project_state_file_not_found (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_file_not_found)\nTest project state loading when file not found ... ok\ntest_load_project_state_invalid_json (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_invalid_json)\nTest project state loading with invalid JSON ... ok\ntest_load_project_state_success (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_success)\nTest successful project state loading ... ok\ntest_shell_initialization (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_shell_initialization)\nTest shell initializes with correct attributes ... ok\ntest_validate_command_phase_args_preview (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_args_preview)\nTest args preview in validation logging ... ok\ntest_validate_command_phase_implementation (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_implementation)\nTest command validation for implementation phase ... ok\ntest_validate_command_phase_performance_tracking (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_performance_tracking)\nTest that command validation tracks performance ... ok\ntest_validate_command_phase_planning (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_planning)\nTest command validation for planning phase ... ok\ntest_validate_command_phase_review (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_review)\nTest command validation for review phase ... ok\ntest_validate_command_phase_unknown (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_unknown)\nTest command validation for unknown phase ... ok\ntest_validate_command_phase_validation (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_validation)\nTest command validation for validation phase ... ok\ntest_validate_command_phase_validation_result (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_validation_result)\nTest validation result logging ... ok\ntest_main_empty_command (tests.unit.test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_empty_command)\nTest main with empty command string ... \u274c Empty command provided\nok\ntest_main_invalid_command_string (tests.unit.test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_invalid_command_string)\nTest main with invalid command string ... \u274c Invalid command string: No closing quotation\nok\ntest_main_no_command (tests.unit.test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_no_command)\nTest main with no command provided ... Usage: logged_secure_shell <command>\nok\n\n----------------------------------------------------------------------\nRan 15 tests in 0.004s\n\nOK\n"
    },
    {
      "name": "tests/unit/test_test_runner_v2.py",
      "success": true,
      "duration": "0.06s",
      "timestamp": "2025-07-22T21:38:50.836137+00:00",
      "metadata": {
        "exit_code": 0,
        "test_file": "tests/unit/test_test_runner_v2.py",
        "module_name": "tests.unit.test_test_runner_v2",
        "layer": "unit",
        "test_count": 22,
        "failures": 0,
        "errors": 0
      },
      "error": "\n============================================================\nRunning: test.sh\n============================================================\n\n\u274c FAILED - test.sh (0.00s)\n\nErrors:\nTest failed\n\n============================================================\nRunning: test.sh\n============================================================\n\n\ud83d\udea8 PERMISSION ERROR - test.sh\nFix with: chmod +x /test/project/test.sh\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u2705 PASSED - test.sh (0.00s)\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u23f1\ufe0f  TIMEOUT - test.sh (300s)\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u2705 PASSED - test.sh (0.00s)\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 3\nPassed:      2 \u2705\nFailed:      1 \u274c\nSuccess Rate: 66.7%\nTotal Time:   4.30s\n\nBy Layer:\n  shell: 1/2 passed\n  python: 1/1 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250722_173850.json\n============================================================\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nLoading plugins from: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\n\nSkipping disabled layer: test\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nRunning shell tests\n============================================================\nFound 2 shell test(s)\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 2\nPassed:      1 \u2705\nFailed:      1 \u274c\nSuccess Rate: 50.0%\nTotal Time:   3.00s\n\nBy Layer:\n  shell: 1/2 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250101_000000.json\n============================================================\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 2\nPassed:      1 \u2705\nFailed:      1 \u274c\nSuccess Rate: 50.0%\nTotal Time:   3.00s\n\nBy Layer:\n  shell: 1/2 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250101_000000.json\n============================================================\n\ntest_discover_tests (tests.unit.test_test_runner_v2.TestShellTestLayer.test_discover_tests)\nTest test discovery ... ok\ntest_discover_tests_multiple_patterns (tests.unit.test_test_runner_v2.TestShellTestLayer.test_discover_tests_multiple_patterns)\nTest discovery with multiple patterns ... ok\ntest_run_test_failure (tests.unit.test_test_runner_v2.TestShellTestLayer.test_run_test_failure)\nTest failed test execution ... ok\ntest_run_test_permission_error (tests.unit.test_test_runner_v2.TestShellTestLayer.test_run_test_permission_error)\nTest permission error handling ... ok\ntest_run_test_success (tests.unit.test_test_runner_v2.TestShellTestLayer.test_run_test_success)\nTest successful test execution ... ok\ntest_run_test_timeout (tests.unit.test_test_runner_v2.TestShellTestLayer.test_run_test_timeout)\nTest test execution timeout ... ok\ntest_run_test_with_input (tests.unit.test_test_runner_v2.TestShellTestLayer.test_run_test_with_input)\nTest that input=\"\" is passed to prevent hangs ... ok\ntest_context_defaults (tests.unit.test_test_runner_v2.TestTestContext.test_context_defaults)\nTest TestContext default values ... ok\ntest_context_initialization (tests.unit.test_test_runner_v2.TestTestContext.test_context_initialization)\nTest TestContext initializes correctly ... ok\ntest_get_layer (tests.unit.test_test_runner_v2.TestTestLayerRegistry.test_get_layer)\nTest getting registered layer ... ok\ntest_get_nonexistent_layer (tests.unit.test_test_runner_v2.TestTestLayerRegistry.test_get_nonexistent_layer)\nTest getting non-existent layer returns None ... ok\ntest_list_layers (tests.unit.test_test_runner_v2.TestTestLayerRegistry.test_list_layers)\nTest listing all layers ... ok\ntest_register_layer (tests.unit.test_test_runner_v2.TestTestLayerRegistry.test_register_layer)\nTest layer registration ... ok\ntest_result_initialization (tests.unit.test_test_runner_v2.TestTestResult.test_result_initialization)\nTest TestResult initializes correctly ... ok\ntest_result_to_dict (tests.unit.test_test_runner_v2.TestTestResult.test_result_to_dict)\nTest TestResult.to_dict method ... ok\ntest_calculate_summary (tests.unit.test_test_runner_v2.TestTestRunner.test_calculate_summary)\nTest summary calculation from results ... ok\ntest_initialization (tests.unit.test_test_runner_v2.TestTestRunner.test_initialization)\nTest runner initialization ... ok\ntest_load_config_missing (tests.unit.test_test_runner_v2.TestTestRunner.test_load_config_missing)\nTest config loading with missing file ... ok\ntest_load_config_success (tests.unit.test_test_runner_v2.TestTestRunner.test_load_config_success)\nTest successful config loading ... ok\ntest_run_layer_disabled (tests.unit.test_test_runner_v2.TestTestRunner.test_run_layer_disabled)\nTest running disabled layer ... ok\ntest_run_layer_success (tests.unit.test_test_runner_v2.TestTestRunner.test_run_layer_success)\nTest running enabled layer ... ok\ntest_save_results (tests.unit.test_test_runner_v2.TestTestRunner.test_save_results)\nTest saving results to JSON ... ok\n\n----------------------------------------------------------------------\nRan 22 tests in 0.006s\n\nOK\n"
    }
  ]
}