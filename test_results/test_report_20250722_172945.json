{
  "summary": {
    "total": 10,
    "passed": 7,
    "failed": 3,
    "success_rate": "70.0%",
    "total_duration": "13.11s",
    "timestamp": "2025-07-22T21:29:45.702746+00:00"
  },
  "by_layer": {
    "shell": {
      "total": 5,
      "passed": 4,
      "failed": 1
    },
    "unit": {
      "total": 5,
      "passed": 3,
      "failed": 2
    }
  },
  "results": [
    {
      "name": "analyze_logs.sh",
      "success": true,
      "duration": "0.00s",
      "timestamp": "2025-07-22T21:29:32.591842+00:00",
      "metadata": {
        "exit_code": 0,
        "script_path": "analyze_logs.sh",
        "layer": "shell"
      },
      "error": "\ud83d\udcca Phase-Driven Development Log Analysis\n========================================\nProject: .\nLogs: ./.logs\n\n\ud83d\udd52 RECENT ACTIVITY (Last 20 events)\n-----------------------------------\nNo automation.log found\n\n\u274c ERROR SUMMARY\n----------------\nNo errors.log found\n\n\ud83d\udd27 COMMAND EXECUTION SUMMARY\n----------------------------\nNo commands.log found\n\n\u26a1 WORKFLOW PROGRESS\n-------------------\nNo workflow.log found\n\n\ud83d\ude80 PERFORMANCE METRICS\n----------------------\nNo performance.log found\n\n\u2705 QUALITY GATES\n---------------\nNo quality-gates.log found\n\n\ud83d\udd17 CORRELATION ANALYSIS\n----------------------\nNo automation.log found\n\n\ud83d\udccb LOG FILES SUMMARY\n-------------------\n\n\ud83d\udd0d DETAILED ANALYSIS COMMANDS\n-----------------------------\nReal-time monitoring:\n  tail -f ./.logs/automation.log | jq .\n  tail -f ./.logs/errors.log | jq .\n\nSpecific analysis:\n  # Find all failed commands:\n  jq 'select(.details.exit_code != 0)' ./.logs/commands.log\n\n  # Performance analysis:\n  jq 'select(.details.duration_ms > 1000)' ./.logs/performance.log\n\n  # Trace specific correlation ID:\n  grep 'your-correlation-id' ./.logs/*.log | jq .\n\n\u2728 Analysis complete. Use the commands above for deeper investigation.\n"
    },
    {
      "name": "install.sh",
      "success": true,
      "duration": "0.01s",
      "timestamp": "2025-07-22T21:29:32.597425+00:00",
      "metadata": {
        "exit_code": 0,
        "script_path": "install.sh",
        "layer": "shell"
      },
      "error": "\ud83d\ude80 Installing Claude Code Project Management Commands...\n\u2705 Commands installed successfully!\n\nAvailable commands:\n  /user:project:setup <project-name>    - Create new project worktree and structure\n  /user:project:doctor                  - Validate project setup\n  /user:project:start                   - Begin automated development\n  /user:project:status                  - Show project progress\n  /user:project:pause                   - Pause automation\n  /user:project:resume                  - Resume automation\n  /user:project:stop                    - End project cleanly\n  /user:project:advance [phase]         - Force phase advancement\n  /user:project:phase <action>          - Manage phases\n\nNext steps:\n1. Navigate to a git repository\n2. Run: /user:project:setup my-project-name\n3. Customize the phase files created\n4. Run: /user:project:doctor\n5. Run: /user:project:start\n\nNote: These commands are designed for use with --dangerously-skip-permissions\nEnsure you understand the risks and operate in isolated git worktrees.\n\ncp: /Users/czei/claude-project-commands/project/*: No such file or directory\n"
    },
    {
      "name": "tests/integration_test.sh",
      "success": true,
      "duration": "6.34s",
      "timestamp": "2025-07-22T21:29:38.940670+00:00",
      "metadata": {
        "exit_code": 0,
        "script_path": "tests/integration_test.sh",
        "layer": "shell"
      },
      "error": "\ud83e\uddea Phase-Driven Development System Integration Test\n==================================================\n\n\u001b[1;33m\u2139\ufe0f  Setting up test environment...\u001b[0m\nInitialized empty Git repository in /Users/czei/ai-software-project-management/test-output/test-repo/.git/\n[main (root-commit) 50a233b] Initial commit\n 1 file changed, 1 insertion(+)\n create mode 100644 README.md\n\u001b[0;32m\u2705 Test environment ready\u001b[0m\n\nTest 1: Project Setup\n--------------------\n\u001b[1;33m\u2139\ufe0f  Creating git worktree...\u001b[0m\nHEAD is now at 50a233b Initial commit\n\u001b[0;32m\u2705 Git worktree created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Creating project structure...\u001b[0m\n\u001b[0;32m\u2705 Project structure created\u001b[0m\n\nTest 2: Phase Execution\n----------------------\n\u001b[1;33m\u2139\ufe0f  Executing Phase 01: Setup...\u001b[0m\n\u001b[0;32m\u2705 Phase 01 outputs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Executing Phase 02: Process...\u001b[0m\n\u001b[0;32m\u2705 Phase 02 outputs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Executing Phase 03: Validate...\u001b[0m\n\u001b[0;32m\u2705 Phase 03 validation complete\u001b[0m\n\nTest 3: State Management\n-----------------------\n\u001b[1;33m\u2139\ufe0f  Testing workflow state transitions...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 1: planning\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 2: implementation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 3: validation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 4: review\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 5: refinement\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 6: integration\u001b[0m\n\u001b[0;32m\u2705 Workflow state transitions tested\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Testing quality gate tracking...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: compilation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: testing\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: review\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: integration\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: documentation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: performance\u001b[0m\n\u001b[0;32m\u2705 Quality gates tracked successfully\u001b[0m\n\nTest 4: Logging Infrastructure\n-----------------------------\n\u001b[1;33m\u2139\ufe0f  Creating structured logs...\u001b[0m\n\u001b[0;32m\u2705 Structured logs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Testing log analysis...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Total log entries:        7\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Correlated events: .logs/automation.log:2\n.logs/commands.log:2\n.logs/performance.log:0\n.logs/quality-gates.log:0\u001b[0m\n\u001b[0;32m\u2705 Logging infrastructure verified\u001b[0m\n\nTest 5: Measurable Outcomes\n--------------------------\n\u001b[1;33m\u2139\ufe0f  Verifying all measurable outcomes...\u001b[0m\n\u001b[0;32m\u2705 File exists: output/setup.txt\u001b[0m\n\u001b[0;32m\u2705 File exists: output/config.json\u001b[0m\n\u001b[0;32m\u2705 File exists: output/metrics.json\u001b[0m\n\u001b[0;32m\u2705 File exists: output/processed_data.csv\u001b[0m\n\u001b[0;32m\u2705 File exists: output/validation_report.txt\u001b[0m\n\u001b[0;32m\u2705 File exists: .project-state.json\u001b[0m\n\u001b[0;32m\u2705 File exists: .workflow-state.json\u001b[0m\n\u001b[0;32m\u2705 All expected files created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Checking measurable changes...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  setup.txt has        3 lines\u001b[0m\n\u001b[0;32m\u2705 config.json is valid JSON\u001b[0m\n\u001b[1;33m\u2139\ufe0f  CSV has        4 lines\u001b[0m\n\u001b[0;32m\u2705 Test summary created\u001b[0m\n\n==================================================\n\u001b[0;32m\u2705 All integration tests completed successfully! \ud83c\udf89\u001b[0m\n\n\u001b[1;33m\u2139\ufe0f  Test artifacts available at: /Users/czei/ai-software-project-management/test-output/test-measurable-project\u001b[0m\n\n\n\u001b[1;33m\u2139\ufe0f  Cleaning up...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Cleaning up test directory...\u001b[0m\n\nPreparing worktree (new branch 'feature/test-measurable-project')\n"
    },
    {
      "name": "tests/run_all_tests.sh",
      "success": true,
      "duration": "6.41s",
      "timestamp": "2025-07-22T21:29:45.354875+00:00",
      "metadata": {
        "exit_code": 0,
        "script_path": "tests/run_all_tests.sh",
        "layer": "shell"
      },
      "error": "\ud83e\uddea Phase-Driven Development System - Full Test Suite\n===================================================\n\n\n\u001b[1;33mRunning: Unit Tests\u001b[0m\n----------------------------------------\n\u001b[0;32m\u2705 Unit Tests passed\u001b[0m\n\n\u001b[1;33mRunning: Command Flow Test\u001b[0m\n----------------------------------------\n\ud83e\uddea Claude Code Command Flow Test\n================================\n\n\n--- Executing: /user:project:setup test-project ---\n\ud83d\udd27 Setting up project: test-project\n\u2705 Project structure created at: /Users/czei/ai-software-project-management/test-output/command-test/test-project\n\n--- Executing: /user:project:doctor  ---\n\ud83d\udd0d Running project doctor...\n\u2705 Project structure\n\u2705 State file\n\u2705 Phase files\n\u2705 Output directory\n\u2705 Logs directory\n\n\u2705 All checks passed! Ready to start.\n\n--- Executing: /user:project:start  ---\n\ud83d\ude80 Starting automated development...\n\u2705 Automation started\n\ud83d\udccd Beginning Phase 01...\n\u2705 Phase 01 outputs created\n\n--- Executing: /user:project:status  ---\n\ud83d\udcca Project Status\n================\nProject: test-project\nStatus: active\nCurrent Phase: 01\nWorkflow Step: planning\nAutomation: ACTIVE\nCompleted Phases: None\n\nOutputs Created: 2\n  - phase01.txt\n  - progress.json\n\n--- Executing: /user:project:pause  ---\n\u23f8\ufe0f  Pausing automation...\n\u2705 Automation paused\n\n--- Executing: /user:project:status  ---\n\ud83d\udcca Project Status\n================\nProject: test-project\nStatus: active\nCurrent Phase: 01\nWorkflow Step: planning\nAutomation: PAUSED\nCompleted Phases: None\n\nOutputs Created: 2\n  - phase01.txt\n  - progress.json\n\n--- Executing: /user:project:resume  ---\n\u25b6\ufe0f  Resuming automation...\n\u2705 Automation resumed\n\n--- Executing: /user:project:update  ---\n\ud83d\udcdd Updating project state...\n\u2705 Updated workflow step to: implementation\n\n--- Executing: /user:project:advance  ---\n\u23ed\ufe0f  Advancing phase...\n\u2705 Advanced from Phase 01 to Phase 02\n\u2705 Phase 02 outputs created\n\n--- Executing: /user:project:status  ---\n\ud83d\udcca Project Status\n================\nProject: test-project\nStatus: active\nCurrent Phase: 02\nWorkflow Step: planning\nAutomation: ACTIVE\nCompleted Phases: 01\n\nOutputs Created: 3\n  - phase01.txt\n  - phase02.txt\n  - progress.json\n\n--- Executing: /user:project:stop  ---\n\ud83c\udfc1 Stopping project...\n\n\ud83d\udcca Project Summary:\n  - Phases Completed: 1\n  - Files Created: 3\n  - Status: COMPLETED\n\n\u2705 Command flow test completed!\nTest artifacts at: /Users/czei/ai-software-project-management/test-output/command-test\n\n[Non-interactive mode: Skipping cleanup prompt]\n\u001b[0;32m\u2705 Command Flow Test passed\u001b[0m\n\n\u001b[1;33mRunning: Integration Test\u001b[0m\n----------------------------------------\n\ud83e\uddea Phase-Driven Development System Integration Test\n==================================================\n\n\u001b[1;33m\u2139\ufe0f  Setting up test environment...\u001b[0m\nInitialized empty Git repository in /Users/czei/ai-software-project-management/test-output/test-repo/.git/\n[main (root-commit) 69e78cf] Initial commit\n 1 file changed, 1 insertion(+)\n create mode 100644 README.md\n\u001b[0;32m\u2705 Test environment ready\u001b[0m\n\nTest 1: Project Setup\n--------------------\n\u001b[1;33m\u2139\ufe0f  Creating git worktree...\u001b[0m\nHEAD is now at 69e78cf Initial commit\n\u001b[0;32m\u2705 Git worktree created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Creating project structure...\u001b[0m\n\u001b[0;32m\u2705 Project structure created\u001b[0m\n\nTest 2: Phase Execution\n----------------------\n\u001b[1;33m\u2139\ufe0f  Executing Phase 01: Setup...\u001b[0m\n\u001b[0;32m\u2705 Phase 01 outputs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Executing Phase 02: Process...\u001b[0m\n\u001b[0;32m\u2705 Phase 02 outputs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Executing Phase 03: Validate...\u001b[0m\n\u001b[0;32m\u2705 Phase 03 validation complete\u001b[0m\n\nTest 3: State Management\n-----------------------\n\u001b[1;33m\u2139\ufe0f  Testing workflow state transitions...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 1: planning\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 2: implementation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 3: validation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 4: review\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 5: refinement\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Workflow step 6: integration\u001b[0m\n\u001b[0;32m\u2705 Workflow state transitions tested\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Testing quality gate tracking...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: compilation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: testing\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: review\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: integration\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: documentation\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Quality gate passed: performance\u001b[0m\n\u001b[0;32m\u2705 Quality gates tracked successfully\u001b[0m\n\nTest 4: Logging Infrastructure\n-----------------------------\n\u001b[1;33m\u2139\ufe0f  Creating structured logs...\u001b[0m\n\u001b[0;32m\u2705 Structured logs created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Testing log analysis...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Total log entries:        7\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Correlated events: .logs/automation.log:2\n.logs/commands.log:2\n.logs/performance.log:0\n.logs/quality-gates.log:0\u001b[0m\n\u001b[0;32m\u2705 Logging infrastructure verified\u001b[0m\n\nTest 5: Measurable Outcomes\n--------------------------\n\u001b[1;33m\u2139\ufe0f  Verifying all measurable outcomes...\u001b[0m\n\u001b[0;32m\u2705 File exists: output/setup.txt\u001b[0m\n\u001b[0;32m\u2705 File exists: output/config.json\u001b[0m\n\u001b[0;32m\u2705 File exists: output/metrics.json\u001b[0m\n\u001b[0;32m\u2705 File exists: output/processed_data.csv\u001b[0m\n\u001b[0;32m\u2705 File exists: output/validation_report.txt\u001b[0m\n\u001b[0;32m\u2705 File exists: .project-state.json\u001b[0m\n\u001b[0;32m\u2705 File exists: .workflow-state.json\u001b[0m\n\u001b[0;32m\u2705 All expected files created\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Checking measurable changes...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  setup.txt has        3 lines\u001b[0m\n\u001b[0;32m\u2705 config.json is valid JSON\u001b[0m\n\u001b[1;33m\u2139\ufe0f  CSV has        4 lines\u001b[0m\n\u001b[0;32m\u2705 Test summary created\u001b[0m\n\n==================================================\n\u001b[0;32m\u2705 All integration tests completed successfully! \ud83c\udf89\u001b[0m\n\n\u001b[1;33m\u2139\ufe0f  Test artifacts available at: /Users/czei/ai-software-project-management/test-output/test-measurable-project\u001b[0m\n\n\n\u001b[1;33m\u2139\ufe0f  Cleaning up...\u001b[0m\n\u001b[1;33m\u2139\ufe0f  Cleaning up test directory...\u001b[0m\n\u001b[0;32m\u2705 Integration Test passed\u001b[0m\n\n===================================================\nTest Summary:\n  \u001b[0;32mPassed: 3\u001b[0m\n  \u001b[0;31mFailed: 0\u001b[0m\n\n\u001b[0;32m\u2705 All tests passed!\u001b[0m\n\ntest_log_file_creation (__main__.TestLoggingInfrastructure.test_log_file_creation)\nTest that all log files are created ... ok\ntest_performance_metrics (__main__.TestLoggingInfrastructure.test_performance_metrics)\nTest performance logging ... /Users/czei/ai-software-project-management/tests/test_phases.py:524: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_structured_logging (__main__.TestLoggingInfrastructure.test_structured_logging)\nTest JSON structured log format ... /Users/czei/ai-software-project-management/tests/test_phases.py:493: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_automation_control (__main__.TestPhaseDrivenDevelopment.test_automation_control)\nTest automation pause/resume functionality ... /Users/czei/ai-software-project-management/tests/test_phases.py:181: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"started\": datetime.utcnow().isoformat() + \"Z\"\nok\ntest_error_recovery (__main__.TestPhaseDrivenDevelopment.test_error_recovery)\nTest error handling and recovery ... ok\ntest_phase_advancement (__main__.TestPhaseDrivenDevelopment.test_phase_advancement)\nTest phase progression logic ... ok\ntest_phase_file_structure (__main__.TestPhaseDrivenDevelopment.test_phase_file_structure)\nTest that phase files have required sections ... ok\ntest_phase_operations (__main__.TestPhaseDrivenDevelopment.test_phase_operations)\nTest actual phase operations (file creation/modification) ... /Users/czei/ai-software-project-management/tests/test_phases.py:311: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  initial_data = {\"version\": \"1.0\", \"created\": datetime.utcnow().isoformat()}\n/Users/czei/ai-software-project-management/tests/test_phases.py:313: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  log_file.write_text(f\"[{datetime.utcnow().isoformat()}] Project initialized\\n\")\n/Users/czei/ai-software-project-management/tests/test_phases.py:328: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  f.write(f\"[{datetime.utcnow().isoformat()}] Processing started\\n\")\n/Users/czei/ai-software-project-management/tests/test_phases.py:342: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  Generated: {datetime.utcnow().isoformat()}\n/Users/czei/ai-software-project-management/tests/test_phases.py:369: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"completed\": datetime.utcnow().isoformat()\nok\ntest_quality_gates (__main__.TestPhaseDrivenDevelopment.test_quality_gates)\nTest quality gate tracking ... ok\ntest_state_management (__main__.TestPhaseDrivenDevelopment.test_state_management)\nTest project state file operations ... ok\ntest_workflow_progression (__main__.TestPhaseDrivenDevelopment.test_workflow_progression)\nTest 6-step workflow state transitions ... ok\n\n----------------------------------------------------------------------\nRan 11 tests in 0.011s\n\nOK\n/Users/czei/ai-software-project-management/tests/test_command_flow.py:77: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"started\": datetime.utcnow().isoformat() + \"Z\"\n/Users/czei/ai-software-project-management/tests/test_command_flow.py:304: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  phase_file.write_text(f\"Phase {phase_num} executed at {datetime.utcnow().isoformat()}\\n\")\n/Users/czei/ai-software-project-management/tests/test_command_flow.py:314: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"completed\": datetime.utcnow().isoformat(),\n/Users/czei/ai-software-project-management/tests/test_command_flow.py:283: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  state[\"completed\"] = datetime.utcnow().isoformat() + \"Z\"\nPreparing worktree (new branch 'feature/test-measurable-project')\n"
    },
    {
      "name": "tests/run_unit_tests.sh",
      "success": false,
      "duration": "0.10s",
      "timestamp": "2025-07-22T21:29:45.454305+00:00",
      "metadata": {
        "exit_code": 1,
        "script_path": "tests/run_unit_tests.sh",
        "layer": "shell"
      },
      "error": "Running unit tests for deterministic components...\n================================================\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u274c FAILED - test.sh (0.00s)\n\nErrors:\nTest failed\n\n============================================================\nRunning: test.sh\n============================================================\n\n\ud83d\udea8 PERMISSION ERROR - test.sh\nFix with: chmod +x /test/project/test.sh\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u2705 PASSED - test.sh (0.00s)\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u23f1\ufe0f  TIMEOUT - test.sh (300s)\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u2705 PASSED - test.sh (0.00s)\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 3\nPassed:      2 \u2705\nFailed:      1 \u274c\nSuccess Rate: 66.7%\nTotal Time:   4.30s\n\nBy Layer:\n  shell: 1/2 passed\n  python: 1/1 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250722_172945.json\n============================================================\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\n\nSkipping disabled layer: test\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nRunning shell tests\n============================================================\nFound 2 shell test(s)\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 2\nPassed:      1 \u2705\nFailed:      1 \u274c\nSuccess Rate: 50.0%\nTotal Time:   3.00s\n\nBy Layer:\n  shell: 1/2 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250101_000000.json\n============================================================\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 2\nPassed:      1 \u2705\nFailed:      1 \u274c\nSuccess Rate: 50.0%\nTotal Time:   3.00s\n\nBy Layer:\n  shell: 1/2 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250101_000000.json\n============================================================\n\n\u274c Unit tests failed with exit code: 1\n\ntest_log_event_console_output (test_basic_logger.TestBasicLogger.test_log_event_console_output)\nTest log event prints to console ... /Users/czei/ai-software-project-management/logged_secure_shell.py:59: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_log_event_invalid_json_state (test_basic_logger.TestBasicLogger.test_log_event_invalid_json_state)\nTest log event handles invalid JSON in state file ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\nok\ntest_log_event_missing_state_file (test_basic_logger.TestBasicLogger.test_log_event_missing_state_file)\nTest log event handles missing state file gracefully ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\nok\ntest_log_event_structure (test_basic_logger.TestBasicLogger.test_log_event_structure)\nTest log event creates proper JSON structure ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event - {\"key\":\"value\"}\nok\ntest_log_event_unknown_category (test_basic_logger.TestBasicLogger.test_log_event_unknown_category)\nTest unknown category defaults to automation log ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] unknown_category: test_event\nok\ntest_log_file_selection (test_basic_logger.TestBasicLogger.test_log_file_selection)\nTest correct log file is selected based on category ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\n[INFO] workflow: test_event\n[INFO] commands: test_event\n[INFO] errors: test_event\n[INFO] performance: test_event\nok\ntest_logger_auto_correlation_id (test_basic_logger.TestBasicLogger.test_logger_auto_correlation_id)\nTest logger generates correlation ID when not provided ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\nok\ntest_logger_initialization (test_basic_logger.TestBasicLogger.test_logger_initialization)\nTest logger initializes with correct attributes ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"custom-id\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\nok\ntest_load_project_state_file_not_found (test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_file_not_found)\nTest project state loading when file not found ... ok\ntest_load_project_state_invalid_json (test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_invalid_json)\nTest project state loading with invalid JSON ... ok\ntest_load_project_state_success (test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_success)\nTest successful project state loading ... ok\ntest_shell_initialization (test_logged_secure_shell.TestLoggedSecureShell.test_shell_initialization)\nTest shell initializes with correct attributes ... ok\ntest_validate_command_phase_args_preview (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_args_preview)\nTest args preview in validation logging ... ok\ntest_validate_command_phase_implementation (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_implementation)\nTest command validation for implementation phase ... ok\ntest_validate_command_phase_performance_tracking (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_performance_tracking)\nTest that command validation tracks performance ... ok\ntest_validate_command_phase_planning (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_planning)\nTest command validation for planning phase ... ok\ntest_validate_command_phase_review (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_review)\nTest command validation for review phase ... ok\ntest_validate_command_phase_unknown (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_unknown)\nTest command validation for unknown phase ... ok\ntest_validate_command_phase_validation (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_validation)\nTest command validation for validation phase ... ok\ntest_validate_command_phase_validation_result (test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_validation_result)\nTest validation result logging ... ok\ntest_main_empty_command (test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_empty_command)\nTest main with empty command string ... \u274c Empty command provided\nok\ntest_main_invalid_command_string (test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_invalid_command_string)\nTest main with invalid command string ... \u274c Invalid command string: No closing quotation\nok\ntest_main_no_command (test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_no_command)\nTest main with no command provided ... Usage: logged_secure_shell <command>\nok\ntest_discover_tests (test_test_runner_v2.TestShellTestLayer.test_discover_tests)\nTest test discovery ... ok\ntest_discover_tests_multiple_patterns (test_test_runner_v2.TestShellTestLayer.test_discover_tests_multiple_patterns)\nTest discovery with multiple patterns ... ok\ntest_run_test_failure (test_test_runner_v2.TestShellTestLayer.test_run_test_failure)\nTest failed test execution ... ok\ntest_run_test_permission_error (test_test_runner_v2.TestShellTestLayer.test_run_test_permission_error)\nTest permission error handling ... ok\ntest_run_test_success (test_test_runner_v2.TestShellTestLayer.test_run_test_success)\nTest successful test execution ... ok\ntest_run_test_timeout (test_test_runner_v2.TestShellTestLayer.test_run_test_timeout)\nTest test execution timeout ... ok\ntest_run_test_with_input (test_test_runner_v2.TestShellTestLayer.test_run_test_with_input)\nTest that input=\"\" is passed to prevent hangs ... ok\ntest_context_defaults (test_test_runner_v2.TestTestContext.test_context_defaults)\nTest TestContext default values ... ok\ntest_context_initialization (test_test_runner_v2.TestTestContext.test_context_initialization)\nTest TestContext initializes correctly ... ok\ntest_get_layer (test_test_runner_v2.TestTestLayerRegistry.test_get_layer)\nTest getting registered layer ... ok\ntest_get_nonexistent_layer (test_test_runner_v2.TestTestLayerRegistry.test_get_nonexistent_layer)\nTest getting non-existent layer returns None ... ok\ntest_list_layers (test_test_runner_v2.TestTestLayerRegistry.test_list_layers)\nTest listing all layers ... ok\ntest_register_layer (test_test_runner_v2.TestTestLayerRegistry.test_register_layer)\nTest layer registration ... ok\ntest_result_initialization (test_test_runner_v2.TestTestResult.test_result_initialization)\nTest TestResult initializes correctly ... ok\ntest_result_to_dict (test_test_runner_v2.TestTestResult.test_result_to_dict)\nTest TestResult.to_dict method ... ok\ntest_calculate_summary (test_test_runner_v2.TestTestRunner.test_calculate_summary)\nTest summary calculation from results ... FAIL\ntest_initialization (test_test_runner_v2.TestTestRunner.test_initialization)\nTest runner initialization ... ok\ntest_load_config_missing (test_test_runner_v2.TestTestRunner.test_load_config_missing)\nTest config loading with missing file ... ok\ntest_load_config_success (test_test_runner_v2.TestTestRunner.test_load_config_success)\nTest successful config loading ... ERROR\ntest_run_layer_disabled (test_test_runner_v2.TestTestRunner.test_run_layer_disabled)\nTest running disabled layer ... ok\ntest_run_layer_success (test_test_runner_v2.TestTestRunner.test_run_layer_success)\nTest running enabled layer ... ok\ntest_save_results (test_test_runner_v2.TestTestRunner.test_save_results)\nTest saving results to JSON ... ok\n\n======================================================================\nERROR: test_load_config_success (test_test_runner_v2.TestTestRunner.test_load_config_success)\nTest successful config loading\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py\", line 722, in mkdir\n    os.mkdir(self, mode)\n    ~~~~~~~~^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/test/project/test_results'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py\", line 722, in mkdir\n    os.mkdir(self, mode)\n    ~~~~~~~~^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/test/project'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/unittest/mock.py\", line 1424, in patched\n    return func(*newargs, **newkeywargs)\n  File \"/Users/czei/ai-software-project-management/tests/unit/test_test_runner_v2.py\", line 305, in test_load_config_success\n    runner = TestRunner(project_root=\"/test/project\")\n  File \"/Users/czei/ai-software-project-management/test_runner_v2.py\", line 253, in __init__\n    self.results_dir.mkdir(parents=True, exist_ok=True)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py\", line 726, in mkdir\n    self.parent.mkdir(parents=True, exist_ok=True)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py\", line 726, in mkdir\n    self.parent.mkdir(parents=True, exist_ok=True)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py\", line 722, in mkdir\n    os.mkdir(self, mode)\n    ~~~~~~~~^^^^^^^^^^^^\nOSError: [Errno 30] Read-only file system: '/test'\n\n======================================================================\nFAIL: test_calculate_summary (test_test_runner_v2.TestTestRunner.test_calculate_summary)\nTest summary calculation from results\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/czei/ai-software-project-management/tests/unit/test_test_runner_v2.py\", line 414, in test_calculate_summary\n    self.assertEqual(report['summary']['total_duration'], \"4.3s\")\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: '4.30s' != '4.3s'\n- 4.30s\n?    -\n+ 4.3s\n\n\n----------------------------------------------------------------------\nRan 45 tests in 0.021s\n\nFAILED (failures=1, errors=1)\n"
    },
    {
      "name": "tests/test_command_flow.py",
      "success": false,
      "duration": "0.03s",
      "timestamp": "2025-07-22T21:29:45.488116+00:00",
      "metadata": {
        "exit_code": 5,
        "test_file": "tests/test_command_flow.py",
        "module_name": "tests.test_command_flow",
        "layer": "unit",
        "test_count": 0,
        "failures": 0,
        "errors": 0
      },
      "error": "\n\n----------------------------------------------------------------------\nRan 0 tests in 0.000s\n\nNO TESTS RAN\n"
    },
    {
      "name": "tests/test_phases.py",
      "success": true,
      "duration": "0.05s",
      "timestamp": "2025-07-22T21:29:45.533508+00:00",
      "metadata": {
        "exit_code": 0,
        "test_file": "tests/test_phases.py",
        "module_name": "tests.test_phases",
        "layer": "unit",
        "test_count": 11,
        "failures": 0,
        "errors": 0
      },
      "error": "\ntest_log_file_creation (tests.test_phases.TestLoggingInfrastructure.test_log_file_creation)\nTest that all log files are created ... ok\ntest_performance_metrics (tests.test_phases.TestLoggingInfrastructure.test_performance_metrics)\nTest performance logging ... /Users/czei/ai-software-project-management/tests/test_phases.py:524: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_structured_logging (tests.test_phases.TestLoggingInfrastructure.test_structured_logging)\nTest JSON structured log format ... /Users/czei/ai-software-project-management/tests/test_phases.py:493: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_automation_control (tests.test_phases.TestPhaseDrivenDevelopment.test_automation_control)\nTest automation pause/resume functionality ... /Users/czei/ai-software-project-management/tests/test_phases.py:181: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"started\": datetime.utcnow().isoformat() + \"Z\"\nok\ntest_error_recovery (tests.test_phases.TestPhaseDrivenDevelopment.test_error_recovery)\nTest error handling and recovery ... ok\ntest_phase_advancement (tests.test_phases.TestPhaseDrivenDevelopment.test_phase_advancement)\nTest phase progression logic ... ok\ntest_phase_file_structure (tests.test_phases.TestPhaseDrivenDevelopment.test_phase_file_structure)\nTest that phase files have required sections ... ok\ntest_phase_operations (tests.test_phases.TestPhaseDrivenDevelopment.test_phase_operations)\nTest actual phase operations (file creation/modification) ... /Users/czei/ai-software-project-management/tests/test_phases.py:311: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  initial_data = {\"version\": \"1.0\", \"created\": datetime.utcnow().isoformat()}\n/Users/czei/ai-software-project-management/tests/test_phases.py:313: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  log_file.write_text(f\"[{datetime.utcnow().isoformat()}] Project initialized\\n\")\n/Users/czei/ai-software-project-management/tests/test_phases.py:328: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  f.write(f\"[{datetime.utcnow().isoformat()}] Processing started\\n\")\n/Users/czei/ai-software-project-management/tests/test_phases.py:342: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  Generated: {datetime.utcnow().isoformat()}\n/Users/czei/ai-software-project-management/tests/test_phases.py:369: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"completed\": datetime.utcnow().isoformat()\nok\ntest_quality_gates (tests.test_phases.TestPhaseDrivenDevelopment.test_quality_gates)\nTest quality gate tracking ... ok\ntest_state_management (tests.test_phases.TestPhaseDrivenDevelopment.test_state_management)\nTest project state file operations ... ok\ntest_workflow_progression (tests.test_phases.TestPhaseDrivenDevelopment.test_workflow_progression)\nTest 6-step workflow state transitions ... ok\n\n----------------------------------------------------------------------\nRan 11 tests in 0.012s\n\nOK\n"
    },
    {
      "name": "tests/unit/test_basic_logger.py",
      "success": true,
      "duration": "0.06s",
      "timestamp": "2025-07-22T21:29:45.592783+00:00",
      "metadata": {
        "exit_code": 0,
        "test_file": "tests/unit/test_basic_logger.py",
        "module_name": "tests.unit.test_basic_logger",
        "layer": "unit",
        "test_count": 8,
        "failures": 0,
        "errors": 0
      },
      "error": "\ntest_log_event_console_output (tests.unit.test_basic_logger.TestBasicLogger.test_log_event_console_output)\nTest log event prints to console ... /Users/czei/ai-software-project-management/logged_secure_shell.py:59: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\nok\ntest_log_event_invalid_json_state (tests.unit.test_basic_logger.TestBasicLogger.test_log_event_invalid_json_state)\nTest log event handles invalid JSON in state file ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\nok\ntest_log_event_missing_state_file (tests.unit.test_basic_logger.TestBasicLogger.test_log_event_missing_state_file)\nTest log event handles missing state file gracefully ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\nok\ntest_log_event_structure (tests.unit.test_basic_logger.TestBasicLogger.test_log_event_structure)\nTest log event creates proper JSON structure ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event - {\"key\":\"value\"}\nok\ntest_log_event_unknown_category (tests.unit.test_basic_logger.TestBasicLogger.test_log_event_unknown_category)\nTest unknown category defaults to automation log ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] unknown_category: test_event\nok\ntest_log_file_selection (tests.unit.test_basic_logger.TestBasicLogger.test_log_file_selection)\nTest correct log file is selected based on category ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\n[INFO] automation: test_event\n[INFO] workflow: test_event\n[INFO] commands: test_event\n[INFO] errors: test_event\n[INFO] performance: test_event\nok\ntest_logger_auto_correlation_id (tests.unit.test_basic_logger.TestBasicLogger.test_logger_auto_correlation_id)\nTest logger generates correlation ID when not provided ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"test-uuid-1234\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\nok\ntest_logger_initialization (tests.unit.test_basic_logger.TestBasicLogger.test_logger_initialization)\nTest logger initializes with correct attributes ... [INFO] automation: logger_initialized - {\"project_dir\":\"/test/project\",\"correlation_id\":\"custom-id\",\"log_files_created\":[\"automation\",\"workflow\",\"commands\",\"quality-gates\",\"phase-transitions\",\"errors\",\"performance\"]}\nok\n\n----------------------------------------------------------------------\nRan 8 tests in 0.011s\n\nOK\n"
    },
    {
      "name": "tests/unit/test_logged_secure_shell.py",
      "success": true,
      "duration": "0.05s",
      "timestamp": "2025-07-22T21:29:45.644069+00:00",
      "metadata": {
        "exit_code": 0,
        "test_file": "tests/unit/test_logged_secure_shell.py",
        "module_name": "tests.unit.test_logged_secure_shell",
        "layer": "unit",
        "test_count": 15,
        "failures": 0,
        "errors": 0
      },
      "error": "\ntest_load_project_state_file_not_found (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_file_not_found)\nTest project state loading when file not found ... ok\ntest_load_project_state_invalid_json (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_invalid_json)\nTest project state loading with invalid JSON ... ok\ntest_load_project_state_success (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_load_project_state_success)\nTest successful project state loading ... ok\ntest_shell_initialization (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_shell_initialization)\nTest shell initializes with correct attributes ... ok\ntest_validate_command_phase_args_preview (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_args_preview)\nTest args preview in validation logging ... ok\ntest_validate_command_phase_implementation (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_implementation)\nTest command validation for implementation phase ... ok\ntest_validate_command_phase_performance_tracking (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_performance_tracking)\nTest that command validation tracks performance ... ok\ntest_validate_command_phase_planning (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_planning)\nTest command validation for planning phase ... ok\ntest_validate_command_phase_review (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_review)\nTest command validation for review phase ... ok\ntest_validate_command_phase_unknown (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_unknown)\nTest command validation for unknown phase ... ok\ntest_validate_command_phase_validation (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_validation)\nTest command validation for validation phase ... ok\ntest_validate_command_phase_validation_result (tests.unit.test_logged_secure_shell.TestLoggedSecureShell.test_validate_command_phase_validation_result)\nTest validation result logging ... ok\ntest_main_empty_command (tests.unit.test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_empty_command)\nTest main with empty command string ... \u274c Empty command provided\nok\ntest_main_invalid_command_string (tests.unit.test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_invalid_command_string)\nTest main with invalid command string ... \u274c Invalid command string: No closing quotation\nok\ntest_main_no_command (tests.unit.test_logged_secure_shell.TestLoggedSecureShellCommandParsing.test_main_no_command)\nTest main with no command provided ... Usage: logged_secure_shell <command>\nok\n\n----------------------------------------------------------------------\nRan 15 tests in 0.004s\n\nOK\n"
    },
    {
      "name": "tests/unit/test_test_runner_v2.py",
      "success": false,
      "duration": "0.06s",
      "timestamp": "2025-07-22T21:29:45.702705+00:00",
      "metadata": {
        "exit_code": 1,
        "test_file": "tests/unit/test_test_runner_v2.py",
        "module_name": "tests.unit.test_test_runner_v2",
        "layer": "unit",
        "test_count": 22,
        "failures": 1,
        "errors": 1
      },
      "error": "\n============================================================\nRunning: test.sh\n============================================================\n\n\u274c FAILED - test.sh (0.00s)\n\nErrors:\nTest failed\n\n============================================================\nRunning: test.sh\n============================================================\n\n\ud83d\udea8 PERMISSION ERROR - test.sh\nFix with: chmod +x /test/project/test.sh\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u2705 PASSED - test.sh (0.00s)\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u23f1\ufe0f  TIMEOUT - test.sh (300s)\n\n============================================================\nRunning: test.sh\n============================================================\n\n\u2705 PASSED - test.sh (0.00s)\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 3\nPassed:      2 \u2705\nFailed:      1 \u274c\nSuccess Rate: 66.7%\nTotal Time:   4.30s\n\nBy Layer:\n  shell: 1/2 passed\n  python: 1/1 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250722_172945.json\n============================================================\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\nPlugin directory does not exist: /test/project/test_layers\n\nSkipping disabled layer: test\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nRunning shell tests\n============================================================\nFound 2 shell test(s)\nPlugin directory does not exist: /test/project/test_layers\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 2\nPassed:      1 \u2705\nFailed:      1 \u274c\nSuccess Rate: 50.0%\nTotal Time:   3.00s\n\nBy Layer:\n  shell: 1/2 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250101_000000.json\n============================================================\n\n\n============================================================\nTEST SUMMARY\n============================================================\nTotal Tests: 2\nPassed:      1 \u2705\nFailed:      1 \u274c\nSuccess Rate: 50.0%\nTotal Time:   3.00s\n\nBy Layer:\n  shell: 1/2 passed\n\nFailed Tests:\n  \u274c test2.sh\n     Failed\n\nDetailed report saved to: /test/project/test_results/test_report_20250101_000000.json\n============================================================\n\ntest_discover_tests (tests.unit.test_test_runner_v2.TestShellTestLayer.test_discover_tests)\nTest test discovery ... ok\ntest_discover_tests_multiple_patterns (tests.unit.test_test_runner_v2.TestShellTestLayer.test_discover_tests_multiple_patterns)\nTest discovery with multiple patterns ... ok\ntest_run_test_failure (tests.unit.test_test_runner_v2.TestShellTestLayer.test_run_test_failure)\nTest failed test execution ... ok\ntest_run_test_permission_error (tests.unit.test_test_runner_v2.TestShellTestLayer.test_run_test_permission_error)\nTest permission error handling ... ok\ntest_run_test_success (tests.unit.test_test_runner_v2.TestShellTestLayer.test_run_test_success)\nTest successful test execution ... ok\ntest_run_test_timeout (tests.unit.test_test_runner_v2.TestShellTestLayer.test_run_test_timeout)\nTest test execution timeout ... ok\ntest_run_test_with_input (tests.unit.test_test_runner_v2.TestShellTestLayer.test_run_test_with_input)\nTest that input=\"\" is passed to prevent hangs ... ok\ntest_context_defaults (tests.unit.test_test_runner_v2.TestTestContext.test_context_defaults)\nTest TestContext default values ... ok\ntest_context_initialization (tests.unit.test_test_runner_v2.TestTestContext.test_context_initialization)\nTest TestContext initializes correctly ... ok\ntest_get_layer (tests.unit.test_test_runner_v2.TestTestLayerRegistry.test_get_layer)\nTest getting registered layer ... ok\ntest_get_nonexistent_layer (tests.unit.test_test_runner_v2.TestTestLayerRegistry.test_get_nonexistent_layer)\nTest getting non-existent layer returns None ... ok\ntest_list_layers (tests.unit.test_test_runner_v2.TestTestLayerRegistry.test_list_layers)\nTest listing all layers ... ok\ntest_register_layer (tests.unit.test_test_runner_v2.TestTestLayerRegistry.test_register_layer)\nTest layer registration ... ok\ntest_result_initialization (tests.unit.test_test_runner_v2.TestTestResult.test_result_initialization)\nTest TestResult initializes correctly ... ok\ntest_result_to_dict (tests.unit.test_test_runner_v2.TestTestResult.test_result_to_dict)\nTest TestResult.to_dict method ... ok\ntest_calculate_summary (tests.unit.test_test_runner_v2.TestTestRunner.test_calculate_summary)\nTest summary calculation from results ... FAIL\ntest_initialization (tests.unit.test_test_runner_v2.TestTestRunner.test_initialization)\nTest runner initialization ... ok\ntest_load_config_missing (tests.unit.test_test_runner_v2.TestTestRunner.test_load_config_missing)\nTest config loading with missing file ... ok\ntest_load_config_success (tests.unit.test_test_runner_v2.TestTestRunner.test_load_config_success)\nTest successful config loading ... ERROR\ntest_run_layer_disabled (tests.unit.test_test_runner_v2.TestTestRunner.test_run_layer_disabled)\nTest running disabled layer ... ok\ntest_run_layer_success (tests.unit.test_test_runner_v2.TestTestRunner.test_run_layer_success)\nTest running enabled layer ... ok\ntest_save_results (tests.unit.test_test_runner_v2.TestTestRunner.test_save_results)\nTest saving results to JSON ... ok\n\n======================================================================\nERROR: test_load_config_success (tests.unit.test_test_runner_v2.TestTestRunner.test_load_config_success)\nTest successful config loading\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py\", line 722, in mkdir\n    os.mkdir(self, mode)\n    ~~~~~~~~^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/test/project/test_results'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py\", line 722, in mkdir\n    os.mkdir(self, mode)\n    ~~~~~~~~^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/test/project'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/unittest/mock.py\", line 1424, in patched\n    return func(*newargs, **newkeywargs)\n  File \"/Users/czei/ai-software-project-management/tests/unit/test_test_runner_v2.py\", line 305, in test_load_config_success\n    runner = TestRunner(project_root=\"/test/project\")\n  File \"/Users/czei/ai-software-project-management/test_runner_v2.py\", line 253, in __init__\n    self.results_dir.mkdir(parents=True, exist_ok=True)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py\", line 726, in mkdir\n    self.parent.mkdir(parents=True, exist_ok=True)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py\", line 726, in mkdir\n    self.parent.mkdir(parents=True, exist_ok=True)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py\", line 722, in mkdir\n    os.mkdir(self, mode)\n    ~~~~~~~~^^^^^^^^^^^^\nOSError: [Errno 30] Read-only file system: '/test'\n\n======================================================================\nFAIL: test_calculate_summary (tests.unit.test_test_runner_v2.TestTestRunner.test_calculate_summary)\nTest summary calculation from results\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/czei/ai-software-project-management/tests/unit/test_test_runner_v2.py\", line 414, in test_calculate_summary\n    self.assertEqual(report['summary']['total_duration'], \"4.3s\")\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: '4.30s' != '4.3s'\n- 4.30s\n?    -\n+ 4.3s\n\n\n----------------------------------------------------------------------\nRan 22 tests in 0.006s\n\nFAILED (failures=1, errors=1)\n"
    }
  ]
}